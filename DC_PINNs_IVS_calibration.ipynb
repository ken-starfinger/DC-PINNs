{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Settings"
      ],
      "metadata": {
        "id": "VK5VpiNhrkCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml_collections"
      ],
      "metadata": {
        "id": "CvhF39iEuAwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V3BjUX8rU9K"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jacrev, jit, lax, random, vmap\n",
        "\n",
        "N = jax.scipy.stats.norm.cdf\n",
        "N_prime = jax.scipy.stats.norm.pdf\n",
        "N_inv = jax.scipy.stats.norm.ppf\n",
        "eps = 1e-15\n",
        "\n",
        "\n",
        "def bound(x): return jnp.maximum(1e-15, x)\n",
        "\n",
        "\n",
        "def d1_(s, k, r, sigma, tau):\n",
        "    s, k, tau, sigma = bound(s), bound(k), bound(tau), bound(sigma)\n",
        "    return (jnp.log(s/k) + (r + sigma*sigma/2)*tau)/(sigma*jnp.sqrt(tau))\n",
        "\n",
        "\n",
        "def d2_(s, k, r, sigma, tau):\n",
        "    s, k, tau, sigma = bound(s), bound(k), bound(tau), bound(sigma)\n",
        "    return d1_(s, k, r, sigma, tau) - sigma*jnp.sqrt(tau)\n",
        "\n",
        "\n",
        "def bs(s, k, tau, r, cp, sigma):\n",
        "    s, k, tau, sigma = bound(s), bound(k), bound(tau), bound(sigma)\n",
        "    d1 = d1_(s, k, r, sigma, tau)\n",
        "    d2 = d2_(s, k, r, sigma, tau)\n",
        "    return cp*s*N(cp*d1) - cp*k*jnp.exp(-r*tau)*N(cp*d2)\n",
        "\n",
        "\n",
        "def black(fwd, k, tau, r, cp, sigma):\n",
        "    fwd, k, tau, sigma = bound(fwd), bound(k), bound(tau), bound(sigma)\n",
        "    return bs(fwd, k, tau, 0., cp, sigma) * jnp.exp(-r*tau)\n",
        "\n",
        "\n",
        "def lv_var(vol, tau):\n",
        "    return (vol **2) * tau\n",
        "\n",
        "\n",
        "def lv_fwd_sqr(k, w, tau, dk_w, d2k_w, dt_w):\n",
        "    # k := ln(k/fwd)\n",
        "    # w := (sigma_BS)^2*tau\n",
        "    w = bound(w)\n",
        "    # see https://quant.stackexchange.com/questions/16343/in-dupires-paper-why-is-s-t-t-in-the-k-t-space\n",
        "    # A = -k / w * dk_w + 0.25 * (-0.25 - 1/w + (k**2)/(w)) * (dk_w **2)\n",
        "    A = -k/w * dk_w + 0.25 * (-0.25 - 1/w + (k**2)/(w**2)) * (dk_w **2)\n",
        "    return dt_w/(1.0 + A + 0.5*d2k_w)\n",
        "\n",
        "\n",
        "def lv_fwd_pde(lv_fwd_sqr, k, d2k_v, dt_v):\n",
        "    return dt_v - 0.5 * lv_fwd_sqr * (k **2) * d2k_v\n",
        "\n",
        "\n",
        "def lv_sqr(s, k, r, v, tau, dk_v, d2k_v, dtau_v):\n",
        "    A = v**2 + 2.0*v*tau*(dtau_v + r*k*dk_v)\n",
        "    y = jnp.log(k/(jnp.exp(r*tau)*s))\n",
        "    B = (1 - k*y/v*dk_v)**2\n",
        "    C = k*v*tau*(dk_v - 0.25*k*v*tau*(dk_v**2) + k*d2k_v)\n",
        "    D = B + C\n",
        "    D = D + (D == 0.)*eps\n",
        "    return A/(B + C)\n",
        "\n",
        "\n",
        "def bs_vega(s, k, tau, r, sigma):\n",
        "    return s*jnp.sqrt(tau)*N_prime(d1_(s, k, r, sigma, tau))\n",
        "\n",
        "\n",
        "def bs_iv(C, s, k, tau, cp, r=0.0,\n",
        "            tol=1e-15, tol_vega=1e-15, ini=0.5,\n",
        "            thr=2.0, max_it=20):\n",
        "    sigma = ini * jnp.ones_like(C)\n",
        "    for i in range(max_it):\n",
        "        diff = bs(s, k, tau, r, cp, sigma) - C\n",
        "        vega = bs_vega(s, k, tau, r, sigma)\n",
        "\n",
        "        end = jnp.logical_or(abs(diff) < tol, vega < tol)\n",
        "        sigma = (sigma - diff / vega) * jnp.logical_not(end) + sigma * end\n",
        "    return jnp.minimum(jnp.maximum(sigma, 0.), thr)\n",
        "\n",
        "\n",
        "def derivatives(fn, x):\n",
        "    def f(x): return fn(x)[0]\n",
        "    def f_dK(x): return grad(f)(x)[0]\n",
        "    dx = vmap(grad(f), 0)(x)\n",
        "    d2x = vmap(grad(f_dK), 0)(x) # CAUTION NOT hessian BUT pdv{pdv[f][x1]}{x}\n",
        "    dx1, d2x1, dx2 = dx.T[0], d2x.T[0], dx.T[1]\n",
        "    return dx1, d2x1, dx2\n",
        "\n",
        "\n",
        "def call_derivatives(fn, x):\n",
        "    def f(x): return bs(s_0, x.T[0], x.T[1], r, 1, fn(x).flatten())[0]\n",
        "    def f_dK(x): return grad(f)(x)[0]\n",
        "    dx = vmap(grad(f), 0)(x)\n",
        "    d2x = vmap(grad(f_dK), 0)(x) # CAUTION NOT hessian BUT pdv{pdv[f][x1]}{x}\n",
        "    dx1, d2x1, dx2 = dx.T[0], d2x.T[0], dx.T[1]\n",
        "    return dx1, d2x1, dx2\n",
        "\n",
        "\n",
        "def error(fn, data):\n",
        "    x_train, y_train, x_mesh = data\n",
        "    K, T = x_mesh[:,0], x_mesh[:,1]\n",
        "\n",
        "    # Calculate derivatives and values\n",
        "    dK_V, d2K_V, dT_V = derivatives(fn, x_mesh)\n",
        "    dK_C, d2K_C, dT_C = call_derivatives(fn, x_mesh)\n",
        "    V = fn(x_mesh).flatten()\n",
        "    C = bs(s_0, K, T, r, 1, V)\n",
        "    LV_sqr = lv_sqr(s_0, K, r, V, T, dK_V, d2K_V, dT_V)\n",
        "\n",
        "    # Error components\n",
        "    pred = bs(s_0, x_train.T[0], x_train.T[1], r, 1, fn(x_train).flatten())\n",
        "    e_acc = (pred - y_train.ravel()) ** 2\n",
        "    e_pde = (dT_C + r*K*dK_C - 0.5*LV_sqr*(K**2)*d2K_C) ** 2\n",
        "    e_arb = {\n",
        "        'dK': jnp.where(dK_C > 0, dK_C**2, jnp.where(dK_C < -jnp.exp(-r*T), dK_C**2, 0)),\n",
        "        'd2K': jnp.where(d2K_C < 0, d2K_C**2, 0),\n",
        "        'dT': jnp.where(dT_C < 0, dT_C**2, 0)\n",
        "    }\n",
        "\n",
        "    return {k: v for k, v in {'e_acc': e_acc, 'e_pde': e_pde, **{'e_arb_'+k: v for k, v in e_arb.items()}}.items()}, \\\n",
        "           {k: jnp.mean(v) for k, v in {'e_acc': e_acc, 'e_pde': e_pde, **{'e_arb_'+k: v for k, v in e_arb.items()}}.items()}\n",
        "\n",
        "\n",
        "def adj(loss, lw, m):\n",
        "    return lw * jnp.mean(m * loss)\n",
        "\n",
        "\n",
        "def make_loss_lb(components):\n",
        "    def loss_fn(fn, data, l_ws, params_sa):\n",
        "        err, metrics = error(fn, data)\n",
        "        loss = {k: adj(err[k], l_ws[k], params_sa[k]) for k in components}\n",
        "        return loss, metrics\n",
        "    return loss_fn\n",
        "\n",
        "\n",
        "loss_fn_lb = {\n",
        "    \"MLP\": make_loss_lb(['e_acc']),\n",
        "    \"PINN\": make_loss_lb(['e_acc', 'e_pde']),\n",
        "    \"DCPINN\": make_loss_lb(['e_acc', 'e_pde', 'e_arb_dK', 'e_arb_d2K', 'e_arb_dT'])\n",
        "}\n",
        "\n",
        "\n",
        "def make_loss_fn(components):\n",
        "    def loss_fn(fn, data, l_ws, params_sa):\n",
        "        loss, metrics = loss_fn_lb[components](fn, data, l_ws, params_sa)\n",
        "        return sum(loss.values()), metrics\n",
        "    return loss_fn\n",
        "\n",
        "\n",
        "loss_fn = {\n",
        "    \"MLP\": make_loss_fn(\"MLP\"),\n",
        "    \"PINN\": make_loss_fn(\"PINN\"),\n",
        "    \"DCPINN\": make_loss_fn(\"DCPINN\")\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "\n",
        "optimizer = \"Adam\"\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "eps = 1e-8\n",
        "learning_rate = 1e-3\n",
        "decay_rate = 0.9\n",
        "decay_steps = 2000\n",
        "grad_accum_steps = 0\n",
        "\n",
        "lr = optax.exponential_decay(\n",
        "        init_value=learning_rate,\n",
        "        transition_steps=decay_steps,\n",
        "        decay_rate=decay_rate,\n",
        "    )\n",
        "tx = optax.adam(\n",
        "    learning_rate=lr, b1=beta1, b2=beta2, eps=eps\n",
        ")"
      ],
      "metadata": {
        "id": "myfni3e7r1UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, random as jran\n",
        "from jax.nn.initializers import glorot_normal, normal, zeros\n",
        "import ml_collections\n",
        "from flax import linen as nn\n",
        "from flax.core.frozen_dict import freeze\n",
        "\n",
        "\n",
        "activation_fn = {\n",
        "    \"tanh\": jnp.tanh,\n",
        "    \"sin\": jnp.sin,\n",
        "}\n",
        "\n",
        "\n",
        "def _get_activation(str):\n",
        "    if str in activation_fn:\n",
        "        return activation_fn[str]\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Activation {str} not supported yet!\")\n",
        "\n",
        "\n",
        "def _weight_fact(init_fn, mean, stddev):\n",
        "    def init(key, shape):\n",
        "        key1, key2 = jran.split(key)\n",
        "        w = init_fn(key1, shape)\n",
        "        g = mean + normal(stddev)(key2, (shape[-1],))\n",
        "        g = jnp.exp(g)\n",
        "        v = w / g\n",
        "        return g, v\n",
        "\n",
        "    return init\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    features: int\n",
        "    kernel_init: Callable = glorot_normal()\n",
        "    bias_init: Callable = zeros\n",
        "    reparam: Union[None, Dict] = None\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        if self.reparam is None:\n",
        "            kernel = self.param(\n",
        "                \"kernel\", self.kernel_init, (x.shape[-1], self.features)\n",
        "            )\n",
        "\n",
        "        elif self.reparam[\"type\"] == \"weight_fact\":\n",
        "            g, v = self.param(\n",
        "                \"kernel\",\n",
        "                _weight_fact(\n",
        "                    self.kernel_init,\n",
        "                    mean=self.reparam[\"mean\"],\n",
        "                    stddev=self.reparam[\"stddev\"],\n",
        "                ),\n",
        "                (x.shape[-1], self.features),\n",
        "            )\n",
        "            kernel = g * v\n",
        "        bias = self.param(\"bias\", self.bias_init, (self.features,))\n",
        "        y = jnp.dot(x, kernel) + bias\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    arch_name: Optional[str]=\"MLP\"\n",
        "    hidden_dim: Tuple[int]=(32, 16)\n",
        "    out_dim: int=1\n",
        "    activation: str=\"tanh\"\n",
        "    periodicity: Union[None, Dict]=None\n",
        "    fourier_emb: Union[None, Dict]=None\n",
        "    reparam: Union[None, Dict]=None\n",
        "\n",
        "    def setup(self):\n",
        "        self.activation_fn = _get_activation(self.activation)\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for i in range(len(self.hidden_dim)):\n",
        "            x = Dense(features=self.hidden_dim[i], reparam=self.reparam)(x)\n",
        "            x = self.activation_fn(x)\n",
        "        x = Dense(features=self.out_dim, reparam=self.reparam)(x)\n",
        "        x = nn.softplus(x) # CAUTION NOT PLAIN MLP\n",
        "        return x\n",
        "\n",
        "\n",
        "class ModifiedMLP(nn.Module):\n",
        "    arch_name: Optional[str]=\"ModifiedMLP\"\n",
        "    hidden_dim: Tuple[int]=(32, 16)\n",
        "    out_dim: int=1\n",
        "    activation: str=\"tanh\"\n",
        "    periodicity: Union[None, Dict]=None\n",
        "    fourier_emb: Union[None, Dict]=None\n",
        "    reparam: Union[None, Dict]=None\n",
        "\n",
        "    def setup(self):\n",
        "        self.activation_fn = _get_activation(self.activation)\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "\n",
        "        u = Dense(features=self.hidden_dim[0], reparam=self.reparam)(x)\n",
        "        v = Dense(features=self.hidden_dim[0], reparam=self.reparam)(x)\n",
        "\n",
        "        u = self.activation_fn(u)\n",
        "        v = self.activation_fn(v)\n",
        "\n",
        "        for i in range(len(self.hidden_dim)):\n",
        "            x = Dense(features=self.hidden_dim[i], reparam=self.reparam)(x)\n",
        "            x = self.activation_fn(x)\n",
        "            x = x * u + (1 - x) * v\n",
        "\n",
        "        x = Dense(features=self.out_dim, reparam=self.reparam)(x)\n",
        "        x = nn.softplus(x) # CAUTION NOT PLAIN MLP\n",
        "        return x\n",
        "\n",
        "\n",
        "def ann_gen(config):\n",
        "    ann = None\n",
        "    reparam = None\n",
        "    if config.ann_reparam==\"weight_fact\":\n",
        "        reparam = ml_collections.ConfigDict({\"type\": \"weight_fact\", \"mean\": 0.5, \"stddev\": 0.1})\n",
        "\n",
        "    if config.ann_str == \"MLP\":\n",
        "        ann = MLP(arch_name=config.ann_str,\n",
        "                  hidden_dim=config.ann_hidden_dim,\n",
        "                  out_dim=config.ann_out_dim,\n",
        "                  activation=config.ann_activation_str,\n",
        "                  periodicity=config.ann_periodicity,\n",
        "                  fourier_emb=config.ann_fourier_emb,\n",
        "                  reparam=reparam)\n",
        "    elif config.ann_str == \"ModifiedMLP\":\n",
        "        ann = ModifiedMLP(arch_name=config.ann_str,\n",
        "                  hidden_dim=config.ann_hidden_dim,\n",
        "                  out_dim=config.ann_out_dim,\n",
        "                  activation=config.ann_activation_str,\n",
        "                  periodicity=config.ann_periodicity,\n",
        "                  fourier_emb=config.ann_fourier_emb,\n",
        "                  reparam=reparam)\n",
        "    return ann"
      ],
      "metadata": {
        "id": "CSp47AkJsG8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Data"
      ],
      "metadata": {
        "id": "8l6Ug6Xdsf97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SABE parameters\n",
        "s_0, r = 1.0, 0.05\n",
        "alpha, beta, rho, nu = 0.3, 0.7, -0.6, 0.6"
      ],
      "metadata": {
        "id": "8sOTFKENsiQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "\n",
        "\n",
        "def get_truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
        "    return truncnorm(\n",
        "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
        "\n",
        "\n",
        "def black(F, K, T, r, sigma):\n",
        "    if T <= 0. or K <= 0.:\n",
        "        return np.exp(-r*T)*max(F - K, 0.)\n",
        "    call = F*N(d1_(F, K, 0., sigma, T)) - K*N(d2_(F, K, 0., sigma, T))\n",
        "    return call*np.exp(-r*T)\n",
        "\n",
        "\n",
        "def chi(z, rho):\n",
        "    if rho != 1.:\n",
        "        s = math.sqrt(1. - 2*rho*z + z*z)\n",
        "        return math.log((s + z - rho) / (1 - rho))\n",
        "    else:\n",
        "        return math.log(1 + z / abs(1 - z))\n",
        "\n",
        "\n",
        "def SabrVolHagan(F, K, T, alpha, beta, nu, rho, h):\n",
        "    coef_vol = 0.\n",
        "    if nu == 0. and rho == 0.:\n",
        "        return alpha\n",
        "    if K <= 0.:\n",
        "        return 0.\n",
        "    FK = F*K\n",
        "    subBeta = 1. - beta\n",
        "    if abs(F - K) > 1e-4:\n",
        "        logFK = math.log(F / K)\n",
        "        a0 = pow(subBeta, 2) / 24*pow(logFK, 2)\n",
        "        a1 = pow(subBeta, 4) / 1920*pow(logFK, 4)\n",
        "        c0 = pow(F*K, (subBeta) / 2)*(1 + a0 + a1)\n",
        "\n",
        "        z = nu / alpha*pow(F*K, (subBeta) / 2)*logFK\n",
        "        c1 = z / chi(z, rho)*math.log((F + h) / (K + h)) / logFK\n",
        "        coef_vol = alpha / c0*c1\n",
        "    else:\n",
        "        coef_vol = alpha*pow(F, beta) / (F + h)\n",
        "    FK_subBeta = pow(FK, subBeta)\n",
        "    sqrtFK = math.sqrt(FK)\n",
        "    y0 = pow(subBeta, 2) / 24.*pow(alpha, 2) / FK_subBeta\n",
        "    y1 = 0.25*(alpha*beta*rho*nu) / pow(FK, (subBeta) / 2)\n",
        "    y2 = (2 - 3.*pow(rho, 2)) / 24.*pow(nu, 2)\n",
        "    y3 = h*(2.*sqrtFK + h)*pow(alpha, 2)\n",
        "    y4 = 24.*pow(sqrtFK + h, 2)*FK_subBeta\n",
        "\n",
        "    return coef_vol*(1 + (y0 + y1 + y2 - y3 / y4)*T)\n",
        "\n",
        "\n",
        "def get_data(n_pts, n_h):\n",
        "    eps = 1e-3\n",
        "\n",
        "    xs_mesh, ts_mesh = np.meshgrid(np.linspace(eps, 2.5+eps, n_h), np.linspace(eps, 5.0+eps, n_h))\n",
        "    x_mesh = np.array([xs_mesh.flatten(), ts_mesh.flatten()]).T\n",
        "    x_val = x_mesh\n",
        "\n",
        "    x_train = np.array([get_truncated_normal(mean=s_0, sd=0.4, low=eps, upp=2.5).rvs(n_pts),\n",
        "                       get_truncated_normal(mean=eps, sd=2, low=eps, upp=5).rvs(n_pts)]).T\n",
        "    df = []\n",
        "    for K, tau in x_train:\n",
        "        F = s_0*np.exp(r*tau)\n",
        "        vol = SabrVolHagan(F, K, tau, alpha, beta, nu, rho, 0.0)*(1 + np.random.normal(0, 0.1))\n",
        "        prem = black(F, K, tau, r, vol)\n",
        "        df.append(prem)\n",
        "    y_train = np.array(df)[...,np.newaxis]\n",
        "    data = jnp.array(x_train), jnp.array(y_train), jnp.array(x_mesh)\n",
        "    return  data"
      ],
      "metadata": {
        "id": "PSCVKjDIsdov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt# input examples\n",
        "\n",
        "data = get_data(300, 101)\n",
        "\n",
        "# premium surfaces\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 4),\n",
        "                    subplot_kw=dict(projection='3d'))\n",
        "fig.suptitle(f'premium surface input')\n",
        "ax.scatter(data[0][:, 0], data[0][:, 1], data[1], s=2)\n",
        "ax.set_xlim(0, 2.5); ax.set_ylim(0, 5.); ax.set_zlim(0.0, 1)\n",
        "plt.show()\n",
        "plt.savefig('img_prem_input.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "U3ALkXr_uXQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calibration"
      ],
      "metadata": {
        "id": "qgXxdr3ZsYmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jacrev, jit, lax, random\n",
        "from jax.flatten_util import ravel_pytree\n",
        "from jax.tree_util import tree_leaves, tree_map\n",
        "import jax.example_libraries.optimizers as optimizers\n",
        "\n",
        "import ml_collections\n",
        "from flax import linen\n",
        "from flax.training import train_state, orbax_utils\n",
        "from flax.serialization import to_state_dict, from_state_dict\n",
        "import orbax.checkpoint\n",
        "\n",
        "\n",
        "def save_params(params, path_item: str) -> None:\n",
        "    serialized_params = to_state_dict(params)\n",
        "    with open(path_item, 'wb') as f:\n",
        "        pickle.dump(serialized_params, f)\n",
        "\n",
        "def load_params(params_initialized, path_item: str):\n",
        "    with open(path_item, 'rb') as f:\n",
        "        loaded_dict = pickle.load(f)\n",
        "    return from_state_dict(loaded_dict, params_initialized)\n",
        "\n",
        "def flatten_pytree(pytree):\n",
        "    return ravel_pytree(pytree)[0]\n",
        "\n",
        "init_l_ws = {\n",
        "    \"MLP\": {'e_acc': 1.},\n",
        "    \"PINN\": {'e_acc': 1., 'e_pde': 1.},\n",
        "    \"DCPINN\": {'e_acc': 1., 'e_pde': 1., 'e_arb_dK': 1., 'e_arb_d2K': 1., 'e_arb_dT': 1.},\n",
        "}\n",
        "\n",
        "def init_params_sa(loss_str, data):\n",
        "    ret = {\n",
        "        \"MLP\": {'e_acc': jnp.ones(len(data[0]))},\n",
        "        \"PINN\": {'e_acc': jnp.ones(len(data[0])),\n",
        "             'e_pde': jnp.ones(len(data[2]))},\n",
        "        \"DCPINN\": {'e_acc': jnp.ones(len(data[0])),\n",
        "             'e_pde': jnp.ones(len(data[2])),\n",
        "             'e_arb_dK': jnp.ones(len(data[2])),\n",
        "             'e_arb_d2K': jnp.ones(len(data[2])),\n",
        "                'e_arb_dT': jnp.ones(len(data[2]))}\n",
        "        }\n",
        "    return ret[loss_str]\n",
        "\n",
        "def calibration(config, data):\n",
        "    ann = ann_gen(config)\n",
        "    ofunc = loss_fn[config.loss_str]\n",
        "\n",
        "    l_ws = init_l_ws[config.loss_str]\n",
        "    params_sa = init_params_sa(config.loss_str, data)\n",
        "\n",
        "    key = jax.random.PRNGKey(config.seed)\n",
        "    key, key_init = jax.random.split(key, 2)\n",
        "    dummy = jnp.ones((1, config.ann_in_dim), dtype=jnp.float32)\n",
        "    state = train_state.TrainState.create(apply_fn=ann.apply,\n",
        "                                        params=ann.init(key_init, dummy),\n",
        "                                        tx=tx)\n",
        "\n",
        "    # self-adaptive\n",
        "    opt_init_sa, opt_update_sa, get_params_sa = optimizers.sgd(1.0)\n",
        "    state_sa = opt_init_sa(params_sa)\n",
        "\n",
        "    @jit\n",
        "    def train_step(state, data, l_ws, state_sa):\n",
        "        params_sa = get_params_sa(state_sa)\n",
        "        def loss_fn(params):\n",
        "            def fn(x): return ann.apply(params, x)\n",
        "            return ofunc(fn, data, l_ws, params_sa)\n",
        "\n",
        "        params = state.params\n",
        "        (loss, metric), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
        "        state = state.apply_gradients(grads=grads)\n",
        "\n",
        "        return state, loss, metric, state_sa\n",
        "\n",
        "    hist_loss = []\n",
        "    momentum = config.loss_balancing_momentum\n",
        "    start_time = time.time()\n",
        "\n",
        "    @jit\n",
        "    def train_step_sa(step, state, data, l_ws, state_sa):\n",
        "        params = state.params\n",
        "        def fn(x): return ann.apply(params, x)\n",
        "        def loss_fn_sa(params_sa):\n",
        "            return ofunc(fn, data, l_ws, params_sa)[0]\n",
        "\n",
        "        params_sa = get_params_sa(state_sa)\n",
        "        value_sa, grads_sa = jax.value_and_grad(loss_fn_sa)(params_sa)\n",
        "        for key in grads_sa.keys():\n",
        "            grads_sa[key] *= -1.\n",
        "        state_sa = opt_update_sa(step, grads_sa, state_sa)\n",
        "\n",
        "        return state_sa\n",
        "\n",
        "    @jit\n",
        "    def update_loss_weights(state, data, l_ws, state_sa):\n",
        "        params = state.params\n",
        "        params_sa = get_params_sa(state_sa)\n",
        "        def loss_fn(params):\n",
        "            def fn(x): return ann.apply(params, x)\n",
        "            return loss_fn_lb[config.loss_str](fn, data, l_ws, params_sa)[0]\n",
        "        grads = jacrev(loss_fn)(params)\n",
        "\n",
        "        # Compute the grad norm of each loss\n",
        "        grad_norm_dict, mean_nonzero_grad_norm_dict = {},{}\n",
        "        for key, value in grads.items():\n",
        "            flattened_grad = flatten_pytree(value)\n",
        "            # grad_norm_dict[key] = jnp.linalg.norm(flattened_grad)\n",
        "            grad_norm_dict[key] = jnp.abs(flattened_grad).mean()\n",
        "\n",
        "\n",
        "        # Compute the mean of grad norms over all losses\n",
        "        sum_grad_norm = jnp.sum(jnp.stack(tree_leaves(grad_norm_dict)))\n",
        "        # Grad Norm Weighting\n",
        "        w = tree_map(lambda x: jnp.where(x==0., 1., sum_grad_norm / x), grad_norm_dict)\n",
        "\n",
        "        running_average = (\n",
        "            lambda old_w, new_w: old_w * momentum + (1 - momentum) * new_w\n",
        "        )\n",
        "        weights = tree_map(running_average, l_ws, w)\n",
        "        weights = lax.stop_gradient(weights)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"{config.loss_str} calibration------>\")\n",
        "    for epoch in range(config.num_epochs):\n",
        "\n",
        "        # Whack-a-mole Learning\n",
        "        if config.loss_str==\"DCPINN\" and epoch % 100 == 0:\n",
        "            l_ws = update_loss_weights(state, data, l_ws, state_sa)\n",
        "        if config.loss_str==\"DCPINN\" and (epoch+50) % 100 == 0:\n",
        "            state_sa = train_step_sa(epoch, state, data, l_ws, state_sa)\n",
        "\n",
        "        state, loss, metric, state_sa = train_step(state, data, l_ws, state_sa)\n",
        "\n",
        "        # Print progress every 1000 epochs\n",
        "        if (epoch % 1000) == 0:\n",
        "            print(f\"Epoch {epoch}: loss = {loss:.6f}\", end=\"\\r\")\n",
        "            hist_loss.append((epoch, float(loss), metric))\n",
        "\n",
        "    comp_time = time.time() - start_time\n",
        "    print(f\"------> completed in {comp_time:.2f} seconds\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    CKPT_DIR = 'checkpoints'\n",
        "    CKPT_DIR = os.path.abspath(CKPT_DIR)\n",
        "    ckpt = {'params': state.params, 'ms': get_params_sa(state_sa), 'ls': l_ws}\n",
        "    orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
        "    save_args = orbax_utils.save_args_from_target(ckpt)\n",
        "    orbax_checkpointer.save(CKPT_DIR, ckpt, force=True, save_args=save_args)\n",
        "\n",
        "    def fn(x): return ann.apply(state.params, x)\n",
        "    return fn, hist_loss\n",
        "\n",
        "def run_experiment(config):\n",
        "    data = get_data(config.pts_num, 101)\n",
        "    model, hist_loss = calibration(config, data)\n",
        "\n",
        "    # Save results\n",
        "    # timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results = {\n",
        "        'config': config,\n",
        "        'history': hist_loss,\n",
        "    }\n",
        "    with open(f'results_latest.pkl', 'wb') as f:\n",
        "        pickle.dump(results, f)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GxakA5tIsV1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example configuration\n",
        "config_MLP = ml_collections.ConfigDict({\n",
        "    \"data_source\": \"SABR_syn\",\n",
        "    \"pts_num\": 300,\n",
        "    \"ann_str\": \"MLP\",\n",
        "    \"loss_str\": \"MLP\",\n",
        "    \"num_epochs\": 5000,\n",
        "    \"ann_in_dim\": 2,\n",
        "    \"ann_out_dim\": 1,\n",
        "    \"ann_activation_str\": \"tanh\",\n",
        "    \"self_adaptive_lr\": 1.0,\n",
        "    \"loss_balancing_momentum\": 0.5,\n",
        "    \"seed\": 42,\n",
        "    \"ann_periodicity\": None,\n",
        "    \"ann_fourier_emb\": None,\n",
        "    \"ann_reparam\": False,\n",
        "    \"ann_hidden_dim\": (16,16,16,16)\n",
        "})\n",
        "\n",
        "config_DCPINN = ml_collections.ConfigDict({\n",
        "    \"data_source\": \"SABR_syn\",\n",
        "    \"pts_num\": 300,\n",
        "    \"ann_str\": \"MLP\",\n",
        "    \"loss_str\": \"DCPINN\",\n",
        "    \"num_epochs\": 5000,\n",
        "    \"ann_in_dim\": 2,\n",
        "    \"ann_out_dim\": 1,\n",
        "    \"ann_activation_str\": \"tanh\",\n",
        "    \"self_adaptive_lr\": 1.0,\n",
        "    \"loss_balancing_momentum\": 0.5,\n",
        "    \"seed\": 42,\n",
        "    \"ann_periodicity\": None,\n",
        "    \"ann_fourier_emb\": None,\n",
        "    \"ann_reparam\": False,\n",
        "    \"ann_hidden_dim\": (16,16,16,16)\n",
        "})\n",
        "\n",
        "\n",
        "# Run single experiment\n",
        "model_MLP = run_experiment(config_MLP)\n",
        "model_DCPINN = run_experiment(config_DCPINN)"
      ],
      "metadata": {
        "id": "ZwSq38RBsSiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "mO_pYcHOsKsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "\n",
        "def load_model(checkpoint_path, config):\n",
        "    \"\"\"Load a saved model from checkpoint\"\"\"\n",
        "    checkpoint_path = os.path.abspath(checkpoint_path)\n",
        "\n",
        "    orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
        "    ckpt = orbax_checkpointer.restore(checkpoint_path)\n",
        "    params = ckpt['params']\n",
        "    params_sa = ckpt['ms']\n",
        "\n",
        "    nn = ann_gen(config)\n",
        "    key = jax.random.PRNGKey(config.seed)\n",
        "    key, key_init = jax.random.split(key, 2)\n",
        "    dummy = jnp.ones((1, config.ann_in_dim), dtype=jnp.float32)\n",
        "    def fn(x): return nn.apply(params, x)\n",
        "    return fn, params_sa\n",
        "\n",
        "def plot_volatility_surface(model, config, save_path=None):\n",
        "    \"\"\"Plot the volatility surface\"\"\"\n",
        "    # Generate grid points\n",
        "    Ks_val, Ts_val = np.meshgrid(np.linspace(0, 2.5, 101), np.linspace(0, 5, 101))\n",
        "    x_val = np.array([Ks_val.flatten(), Ts_val.flatten()]).T\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 4), subplot_kw=dict(projection='3d'))\n",
        "    fig.suptitle(f'Volatility Surface ({config.loss_str})')\n",
        "\n",
        "    # Plot surface\n",
        "    vol_surface = model(x_val).reshape(101, 101)\n",
        "    surf = ax.plot_surface(Ks_val, Ts_val, vol_surface,\n",
        "                          cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
        "\n",
        "    # Customize plot\n",
        "    ax.set_xlabel('Strike')\n",
        "    ax.set_ylabel('Time')\n",
        "    # ax.set_zlabel('Volatility')\n",
        "    ax.view_init(30, -70)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_arbitrage_heatmaps(model, config, save_path=None):\n",
        "    \"\"\"Plot heatmaps showing arbitrage conditions\"\"\"\n",
        "    # Generate dense grid for detailed visualization\n",
        "    K_mesh = np.linspace(0, 2.5, 201)\n",
        "    T_mesh = np.linspace(0, 5, 201)\n",
        "    Ks_mesh, Ts_mesh = np.meshgrid(K_mesh, T_mesh)\n",
        "    x_mesh = np.array([Ks_mesh.flatten(), Ts_mesh.flatten()]).T\n",
        "    K, T = x_mesh[:,0], x_mesh[:,1]\n",
        "\n",
        "    # Calculate derivatives\n",
        "    dK_C, d2K_C, dT_C = call_derivatives(model, x_mesh)\n",
        "    e_arb = {\n",
        "    'dK': jnp.where(dK_C > 0, dK_C**2, jnp.where(dK_C < -jnp.exp(-r*T), dK_C**2, 0)),\n",
        "    'd2K': jnp.where(d2K_C < 0, d2K_C**2, 0),\n",
        "    'dT': jnp.where(dT_C < 0, dT_C**2, 0)}\n",
        "\n",
        "    # Create heatmap plots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
        "    fig.suptitle(f'Arbitrage Condition Heatmaps ({config.loss_str})')\n",
        "\n",
        "    # First derivative wrt K\n",
        "    im1 = axes[0].pcolormesh(Ks_mesh, Ts_mesh,\n",
        "                            e_arb['dK'].reshape(len(T_mesh), len(K_mesh)),\n",
        "                            cmap='bwr',\n",
        "                            vmin=-1e-50, vmax=1e-50, shading='auto', alpha=0.7)\n",
        "    axes[0].set_title('∂C/∂K')\n",
        "    plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "    # Second derivative wrt K\n",
        "    im2 = axes[1].pcolormesh(Ks_mesh, Ts_mesh,\n",
        "                            e_arb['d2K'].reshape(len(T_mesh), len(K_mesh)),\n",
        "                            cmap='bwr',\n",
        "                            vmin=-1e-50, vmax=1e-50, shading='auto', alpha=0.7)\n",
        "    axes[1].set_title('∂²C/∂K²')\n",
        "    plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "    # Derivative wrt T\n",
        "    im3 = axes[2].pcolormesh(Ks_mesh, Ts_mesh,\n",
        "                            e_arb['dT'].reshape(len(T_mesh), len(K_mesh)),\n",
        "                            cmap='bwr',\n",
        "                            vmin=-1e-50, vmax=1e-50, shading='auto', alpha=0.7)\n",
        "    axes[2].set_title('∂C/∂T')\n",
        "    plt.colorbar(im3, ax=axes[2])\n",
        "\n",
        "    # Set labels and adjust layout\n",
        "    for ax in axes:\n",
        "        ax.set_xlabel('Strike')\n",
        "        ax.set_ylabel('Time')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_training_history(history_path, save_path=None):\n",
        "    \"\"\"Plot training loss history\"\"\"\n",
        "    # Load history\n",
        "    with open(history_path, 'rb') as f:\n",
        "        results = pickle.load(f)\n",
        "    history = results['history']\n",
        "\n",
        "    # Extract data\n",
        "    epochs, losses, metrics = zip(*history)\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
        "    ax.plot(epochs, losses, 'b-', label='Total Loss')\n",
        "\n",
        "    # Plot individual metrics if available\n",
        "    colors = {'e_acc': 'r', 'e_pde': 'g', 'e_arb_dK': 'm', 'e_arb_d2K': 'c', 'e_arb_dT': 'y'}\n",
        "    for key in metrics[0].keys():\n",
        "        metric_values = [m[key] for m in metrics]\n",
        "        ax.plot(epochs, metric_values, f'{colors.get(key, \"k\")}--', label=key)\n",
        "\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_yscale('log')\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "    plt.title('Training History')\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def compare_with_sabr(model, save_path=None):\n",
        "    \"\"\"Compare learned volatility surface with true SABR surface\"\"\"\n",
        "    # Generate grid points\n",
        "    x_mesh = np.linspace(0.0, 2.5, 101)\n",
        "    t_mesh = np.linspace(0.0, 5.0, 101)\n",
        "    xs_mesh, ts_mesh = np.meshgrid(x_mesh, t_mesh)\n",
        "    x_vec = np.array([xs_mesh.flatten(), ts_mesh.flatten()]).T\n",
        "\n",
        "    # Calculate SABR volatilities\n",
        "    sabr_vols = []\n",
        "    for K, tau in x_vec:\n",
        "        F = s_0 * np.exp(r * tau)\n",
        "        if K == 0. or tau == 0.:\n",
        "            vol = np.nan\n",
        "        else:\n",
        "            vol = SabrVolHagan(F, K, tau, alpha, beta, nu, rho, 0.0)\n",
        "        sabr_vols.append(vol)\n",
        "    sabr_vols = np.array(sabr_vols)\n",
        "\n",
        "    # Calculate model volatilities\n",
        "    model_vols = model(x_vec).flatten()\n",
        "\n",
        "    # Create figure\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5), subplot_kw=dict(projection='3d'))\n",
        "    fig.suptitle('SABR vs Learned Volatility Surface')\n",
        "\n",
        "    # Plot SABR surface\n",
        "    surf1 = ax1.plot_surface(xs_mesh, ts_mesh,\n",
        "                            sabr_vols.reshape(len(t_mesh), len(x_mesh)),\n",
        "                            cmap=cm.coolwarm)\n",
        "    ax1.set_title('SABR')\n",
        "    plt.colorbar(surf1, ax=ax1)\n",
        "\n",
        "    # Plot learned surface\n",
        "    surf2 = ax2.plot_surface(xs_mesh, ts_mesh,\n",
        "                            model_vols.reshape(len(t_mesh), len(x_mesh)),\n",
        "                            cmap=cm.coolwarm)\n",
        "    ax2.set_title('Calibrated')\n",
        "    plt.colorbar(surf2, ax=ax2)\n",
        "\n",
        "    # Plot difference\n",
        "    diff = model_vols - sabr_vols\n",
        "    surf3 = ax3.plot_surface(xs_mesh, ts_mesh,\n",
        "                            diff.reshape(len(t_mesh), len(x_mesh)),\n",
        "                            cmap=cm.coolwarm)\n",
        "    ax3.set_title('Difference')\n",
        "    plt.colorbar(surf3, ax=ax3)\n",
        "\n",
        "    # Customize plots\n",
        "    for ax in (ax1, ax2, ax3):\n",
        "        ax.set_xlabel('Strike')\n",
        "        ax.set_ylabel('Time')\n",
        "        ax.set_zlabel('Volatility')\n",
        "        ax.view_init(30, -70)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "os.makedirs('figures', exist_ok=True)# Create output directory if it doesn't exist\n",
        "\n",
        "plot_volatility_surface(model_MLP, config_MLP, 'figures/volatility_surface_MLP.png')\n",
        "plot_volatility_surface(model_DCPINN, config_DCPINN, 'figures/volatility_surface_DCPINN.png')\n",
        "plot_arbitrage_heatmaps(model_MLP, config_MLP, 'figures/arbitrage_heatmaps_MLP.png')\n",
        "plot_arbitrage_heatmaps(model_DCPINN, config_DCPINN, 'figures/arbitrage_heatmaps_DCPINN.png')\n",
        "# plot_training_history('results_latest.pkl', 'figures/training_history.png')\n",
        "# compare_with_sabr(model, 'figures/sabr_comparison.png')"
      ],
      "metadata": {
        "id": "AlhnxEDGsJqb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}